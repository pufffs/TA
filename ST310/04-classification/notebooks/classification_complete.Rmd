---
title: "Classification"
author: "Joshua Loftus"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(tidyverse)
library(dplyr )
# install.packages("broom") # if necessary
library(broom)
# install.packages("yardstick") # if necessary
#library(yardstick)
install.packages("GGally")
library(GGally)
```

## Generating simulated data

The true model in this case is a logistic regression model.

Check: Can you understand the true data generating process by reading the code?

```{r}
# Setting parameters
set.seed(1)
# Try changing some parameters
n <- 500
p <- 4
sparsity <- 2
nonzero_beta <- 
  rep(1, sparsity) # or e.g. runif(sparsity, min = -1, max = 1)
true_beta <- c(nonzero_beta, rep(0, p - sparsity))
class_shift <- 2.5 # change this

```

```{r}
px
```

```{r message=FALSE}
# Generating simulated data
X <- matrix(rnorm(n*p), nrow = n)
mu <- class_shift + X %*% true_beta
px <- exp(mu)/(1+exp(mu))
Y <- rbinom(n, 1, prob = px)
train_ld <- data.frame(y = as.factor(Y), x = X)
train_ld |>
  select(y, num_range("x.", 1:4)) |>
  ggpairs(progress = F)
```
 
Check: Do these plots fit with your understanding of the true data generating process?

```{r}
# Class (im)balance
table(train_ld$y)
train_ld |> count(y)
```

**Question**: What would the accuracy be for a classifier that always predicts the most common class?

```{r}
const_yhat_success <- mean(train_ld$y == 1)
max(const_yhat_success, 1 - const_yhat_success)
```


## Logistic regression model

### Coefficients

```{r}
c(class_shift, true_beta)
```


```{r}
fit_glm <- glm(y ~ ., family = binomial(), data = train_ld)
fit_glm |> ggcoef()
```

Coefficients on the odds scale:

```{r}
fit_glm |> 
  tidy(exponentiate = TRUE) |> knitr::kable()
```

### Predictions

```{r}
fit_glm_predictions <- augment(fit_glm, type.predict = "response")
fit_glm_predictions |> pull(.fitted) |> mean()
```

**Question**: Does this number look familiar? Why?

**Answer**: It's the proportion of the majority class that we saw before. Any other value here would indicate an overall bias in the predictions of the model.

```{r}
# Note: the event_level option is currently required because
# of a mismatch between glm() and yardstick
fit_glm_predictions |>
  roc_curve(truth = y, .fitted,
            event_level = "second") |>
  autoplot()
```


**Note**: When a classification model outputs class probabilities or numeric scores these can be converted into classifications by setting thresholds or cutoffs. If we keep the model fixed but change the cutoff, we can achieve different trade-offs between false positives and false negatives (or specificity and sensitivity, precision and recall, etc--these are just other names). Plots like this `roc_curve` (see also `?pr_curve`) show all the possible trade-off values for one model.

**Question**: Using curves like this how can we compare different models? Give at least two suggestions.

**Answer**: We could compare at a specific point on the curve, for example if that point corresponds to a certain false positive rate that has some practical or domain-specific importance (e.g. regulations). Or we could compute the area under the curves, e.g. using the `roc_auc` function.

Classify at several different thresholds

```{r}
# Note: true and predicted classes must use same
# label names, hence the factor(as.numeric()) 
higher_cutoff <- const_yhat_success + .05
confusion_matrix_higher <-
  fit_glm_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > higher_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```


```{r}
lower_cutoff <- .2
confusion_matrix_lower <-
  fit_glm_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > lower_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```

```{r}
confusion_matrix_higher
```

```{r}
confusion_matrix_lower
```

```{r}
confusion_matrix_higher |> autoplot(type = "heatmap")
```

```{r}
confusion_matrix_lower |> autoplot(type = "heatmap")
```

Comparing across various metrics:

```{r}
higher_summary <- summary(confusion_matrix_higher) |>
  mutate(higher = .estimate) |>
  select(.metric, higher)
lower_summary <- summary(confusion_matrix_lower) |>
  mutate(lower = .estimate) |>
  select(lower)
cbind(higher_summary, lower_summary) |>
  knitr::kable()
```

Note the `accuracy`, for example, which is the proportion of data where the predicted class and true class are equal.

### Sub-sampling for class balance

```{r}
rare_class_size <- min(table(train_ld$y))
train_subsampled <- train_ld |>
  group_by(y) |>
  sample_n(rare_class_size) |>
  ungroup()
  
fit_glm_subs <- glm(y ~ ., family = binomial(), data = train_subsampled)
```

Does this change coefficients or inferences about them?

```{r}
cbind(
  tidy(fit_glm) |> select(term, estimate, std.error),
  tidy(fit_glm_subs) |> select(estimate, std.error)) |>
  knitr::kable()
```

**Note**: With a smaller sample size, the balanced training data gives us larger `std.errors` for our estimates. (Wider confident intervals, larger p-values).

```{r}
fit_glm_subs_predictions <- augment(fit_glm_subs, type.predict = "response")
fit_glm_subs_predictions |> pull(.fitted) |> mean()
```

Training data classes are balanced so the predictions are also balanced.

```{r}
# Note: the event_level option is currently required because
# of a mismatch between glm() and yardstick
fit_glm_subs_predictions |>
  roc_curve(truth = y, .fitted,
            event_level = "second") |>
  autoplot()
```

Classify at several different thresholds

```{r}
# Note: true and predicted classes must use same
# label names, hence the factor(as.numeric()) 
higher_cutoff <- const_yhat_success + .05
confusion_matrix_subs_higher <-
  fit_glm_subs_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > higher_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```


```{r}
lower_cutoff <- .2
confusion_matrix_subs_lower <-
  fit_glm_subs_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > lower_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```

```{r}
confusion_matrix_subs_higher
```

```{r}
confusion_matrix_subs_lower
```

```{r}
confusion_matrix_subs_higher |> autoplot(type = "heatmap")
```

```{r}
confusion_matrix_subs_lower |> autoplot(type = "heatmap")
```

Comparing across various metrics:

```{r}
higher_subs_summary <- summary(confusion_matrix_subs_higher) |>
  mutate(higher = .estimate) |>
  select(.metric, higher)
lower_subs_summary <- summary(confusion_matrix_subs_lower) |>
  mutate(lower = .estimate) |>
  select(lower)
metrics <- c("accuracy", "bal_accuracy") #, "precision", "recall")
rbind(
cbind(higher_summary, lower_summary) |>
  filter(.metric %in% metrics) |>
  mutate(subsampled = FALSE),
cbind(higher_subs_summary, lower_subs_summary) |>
  filter(.metric %in% metrics) |>
  mutate(subsampled = TRUE)) |>
  knitr::kable()
```

**Question**: Suppose we have trained a model on data that was subsampled to create class balance. Then we choose classification cutoffs to achieve a desired trade-off between precision/recall (or false positives and false negatives), and we do this still while using the subsampled training data. What will happen if we start using that model to classify data from a new or full dataset that hasn't been subsampled?

**Answer**: Predictions on imbalanced data will achieve a different trade-off between false positives and false negatives. To calibrate this we would need to choose a different classification threshold that works on imbalanced data.



# Logistic Regression

**Logistic regression** is a popular statistical model used for binary classification problems. Unlike linear regression, which predicts a continuous value, logistic regression predicts a probability that a given instance belongs to one of two classes. This document provides a detailed mathematical overview of logistic regression.

# 1. Sigmoid Function

The logistic function, also known as the **sigmoid function**, is used to model the relationship between input features and the probability of the positive class:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

where \( z \) is the linear combination of the input features:

$$
z = \mathbf{w}^T \mathbf{x} + b
$$

- \( \mathbf{w} \): Weights of the model.
- \( \mathbf{x} \): Feature vector.
- \( b \): Bias term.

The output of the logistic function is a value between **0 and 1**, representing the predicted probability of the positive class.

# 2. Log-Odds and Linear Relationship

The **log-odds** (or **logit**) is the logarithm of the odds of the positive class. In logistic regression, the log-odds is modeled as a linear function of the input features:

$$
\text{logit}(p) = \log\left( \frac{p}{1 - p} \right) = \mathbf{w}^T \mathbf{x} + b
$$

where \( p \) is the predicted probability of the positive class.

# 3. Cost Function and Maximum Likelihood Estimation

The goal of logistic regression is to find the optimal parameters (weights and bias) that minimize the prediction error. Instead of using **Mean Squared Error**, logistic regression uses the **logistic loss** or **cross-entropy loss** function, which is defined as:

$$
J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$

where:
- \( m \): Number of training examples.
- \( y^{(i)} \): True label of the \( i \)-th training example.
- \( \hat{y}^{(i)} = \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b) \): Predicted probability for the \( i \)-th example.

The cost function represents the **negative log-likelihood** of the data given the model parameters.

# 4. Maximum Likelihood Derivation

To estimate the parameters, we use **Maximum Likelihood Estimation (MLE)**. The likelihood function for logistic regression is:

$$
L(\mathbf{w}, b) = \prod_{i=1}^m \left( \hat{y}^{(i)} \right)^{y^{(i)}} \left( 1 - \hat{y}^{(i)} \right)^{1 - y^{(i)}}
$$

The **log-likelihood** is obtained by taking the logarithm of the likelihood function:

$$
\ell(\mathbf{w}, b) = \sum_{i=1}^m \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$

The **negative log-likelihood** is equivalent to the cost function, which we aim to minimize.

# 5. Gradient Descent

**Gradient descent** is an iterative optimization algorithm used to minimize the cost function. The process is as follows:

1. **Initialize Parameters**: Start with random values for the weights \( \mathbf{w} \) and bias \( b \).
2. **Compute Predictions**: For each training example, compute the predicted probability:
   
   $$
   \hat{y}^{(i)} = \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)
   $$

3. **Compute Cost**: Calculate the cost function \( J(\mathbf{w}, b) \).
4. **Compute Gradients**: Calculate the gradients of the cost function with respect to the weights and bias:
   
   - For the weights \( \mathbf{w} \):

     $$
     \frac{\partial J(\mathbf{w}, b)}{\partial w_j} = \frac{1}{m} \sum_{i=1}^m \left( \hat{y}^{(i)} - y^{(i)} \right) x_j^{(i)}, \quad \text{for } j = 1, 2, \dots, n
     $$

   - For the bias \( b \):

     $$
     \frac{\partial J(\mathbf{w}, b)}{\partial b} = \frac{1}{m} \sum_{i=1}^m \left( \hat{y}^{(i)} - y^{(i)} \right)
     $$

5. **Update Parameters**: Update the weights and bias using a learning rate \( \alpha \):

   $$
   \mathbf{w} := \mathbf{w} - \alpha \frac{\partial J(\mathbf{w}, b)}{\partial \mathbf{w}}
   $$

   $$
   b := b - \alpha \frac{\partial J(\mathbf{w}, b)}{\partial b}
   $$

6. **Repeat**: Repeat steps 2-5 until the cost function converges to a minimum.

# 6. Hessian Matrix and Newton's Method

An alternative optimization method is **Newton's Method**, which uses the **Hessian matrix** (second-order derivatives) to find the minimum of the cost function. The **Hessian matrix** for logistic regression is given by:

$$
H = \frac{1}{m} \sum_{i=1}^m \hat{y}^{(i)} (1 - \hat{y}^{(i)}) \mathbf{x}^{(i)} \mathbf{x}^{(i)T}
$$

The parameter update rule for Newton's method is:

$$
\theta := \theta - H^{-1} \nabla J(\theta)
$$

where \( \theta = [\mathbf{w}, b] \) is the vector of parameters, and \( \nabla J(\theta) \) is the gradient of the cost function.

# 7. Probabilistic Interpretation

Logistic regression can be interpreted as a **probabilistic model**. The predicted probability \( \hat{y} \) represents the conditional probability of the positive class given the input features:

$$
P(y = 1 \mid \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)
$$
Concern of logistic regression: the log-likelihood function is maximized when the predicted probabilities approach 0 or 1 for all data points, which is only possible if the coefficients approach infinity. Have a look at the log-likelihood function, it is maximized either when $\hat y=0$ or $\hat y=1$. The predicted probability is 
$$
P(\hat y =1) =\frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
$$
Hence to maximize the log-likelihood, the optimization process will drive $P(\hat y =1)$ to be 0 or 1. And this will drive $\mathbf{w}$ to be negative or positive infinity.




# SVM
The primal optimization problem for SVM aims to find a weight vector (**w**) and bias (**b**) that maximize the margin between classes while correctly classifying all training points.

The primal problem can be expressed as follows:

$$
\min_{\mathbf{w}, b} \quad \frac{1}{2} \|\mathbf{w}\|^2
$$

Subject to:

$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad \forall i
$$

Where:
- \( \mathbf{w} \) is the weight vector.
- \( b \) is the bias.
- \( y_i \in \{+1, -1\} \) are the class labels.
- \( \mathbf{x}_i \) are the feature vectors of the training data points.

# 2. Lagrangian Formulation and KKT Conditions

To solve this constrained optimization problem, we introduce Lagrange multipliers \( \alpha_i \geq 0 \) for each constraint and form the Lagrangian:

$$
L_P(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^n \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right]
$$

The **Karush-Kuhn-Tucker (KKT) conditions** are used to find the optimal solution. These include:

1. **Stationarity**: \( \nabla_{\mathbf{w}} L_P = 0 \) and \( \nabla_b L_P = 0 \).
   - This gives the optimal weight vector:
     
     $$
     \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i
     $$

2. **Primal Feasibility**: The original constraints must hold for each training point.
3. **Dual Feasibility**: \( \alpha_i \geq 0 \).
4. **Complementary Slackness**: 

   $$
   \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right] = 0, \quad \forall i
   $$

# 3. Deriving the Dual Problem

The dual problem is derived by substituting the expression for \( \mathbf{w} \) back into the Lagrangian, resulting in:

$$
\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$

Subject to:

1. \( \alpha_i \geq 0 \), for all \( i \).
2. \( \sum_{i=1}^n \alpha_i y_i = 0 \).

# 4. Solving the Dual Problem

The dual problem is a **quadratic programming (QP)** problem involving a quadratic function and linear constraints. Various solvers are used to solve this problem, including **Sequential Minimal Optimization (SMO)** and libraries such as **LIBSVM** or **scikit-learn** in Python.

The solution yields **support vectors** for which \( \alpha_i > 0 \), defining the optimal hyperplane.

# 5. Computing the Optimal Weight Vector and Bias

Once we have the optimal values for \( \alpha_i \), we can compute:

- **Weight Vector**:
  
  $$
  \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i
  $$

- **Bias Term** \( b \): Use any support vector \( \mathbf{x}_s \) (where \( \alpha_s > 0 \)):

  $$
  b = y_s - \mathbf{w}^T \mathbf{x}_s
  $$

# 6. Decision Function

The decision function for classifying a new point \( \mathbf{x} \) is:

$$
 f(\mathbf{x}) = \text{sign}\left( \mathbf{w}^T \mathbf{x} + b \right) = \text{sign}\left( \sum_{i=1}^n \alpha_i y_i (\mathbf{x}_i^T \mathbf{x}) + b \right)
$$

# 7. The Kernel Trick

For non-linear data, we use the **kernel trick** to map data into a higher-dimensional space without explicitly transforming it. The dot product \( \mathbf{x}_i^T \mathbf{x}_j \) is replaced by a **kernel function** \( K(\mathbf{x}_i, \mathbf{x}_j) \):

- **Linear Kernel**: \( K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j \)
- **Polynomial Kernel**: \( K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T \mathbf{x}_j + c)^d \)
- **RBF Kernel**: \( K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left( -\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2} \right) \)

The decision function then becomes:

$$
 f(\mathbf{x}) = \text{sign}\left( \sum_{i=1}^n \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b \right)
$$

# 8. Conclusion

In this document, we outlined the entire SVM optimization process, from the primal and dual formulations to the use of kernel functions for non-linear decision boundaries. The dot product \( \mathbf{x}_i^T \mathbf{x}_j \) plays a key role in determining the similarity between feature vectors, and the kernel trick allows SVM to efficiently find non-linear boundaries.

The **SVM** is a powerful tool that provides robust classification by leveraging **support vectors** and ensuring maximum margin separation between classes.