# Display final weights
cat("Final Weights:\n")
print(final_weights)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:epochs) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, X_batch)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
final_weights <- result$weights
loss_history <- result$loss_history
# Display final weights
cat("Final Weights:\n")
print(final_weights)
X_test <- as.matrix(test_data[, c("Feature1", "Feature2")])
y_test <- test_data$Class
# Add intercept term
X_test <- cbind(Intercept = 1, X_test)
# Compute predictions
z_test <- X_test %*% final_weights
y_hat_test <- sigmoid(z_test)
predictions_test <- ifelse(y_hat_test >= 0.5, 1, 0)
# Calculate accuracy
accuracy_test <- mean(predictions_test == y_test)
beta0 <- final_weights[1]
beta1 <- final_weights[2]
beta2 <- final_weights[3]
# Define decision boundary function
decision_boundary <- function(x1) {
-(beta0 + beta1 * x1) / beta2
}
x1_vals <- seq(min(train_data$Feature1) - 1, max(train_data$Feature1) + 1, length.out = 100)
x2_vals <- decision_boundary(x1_vals)
ggplot(test_data, aes(x = Feature1, y = Feature2, color = factor(Class))) +
geom_point(alpha = 0.7) +
# Add the decision boundary using a separate data frame
geom_line(data = data.frame(Feature1 = x1_vals, Feature2 = x2_vals),
aes(x = Feature1, y = Feature2),
color = "black", size = 1) +
labs(title = "Decision Boundary on Testing Data",
x = "Feature 1",
y = "Feature 2",
color = "Class") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyverse)
theme_set(theme_minimal(base_size = 12))
f <- function(x) x^2 # Function to optimize
grad_f <- function(x) 2 * x # Gradient of f
max_steps <- 15
x0 <- 3 # starting point
gamma <- .995
# ... (rest of the code not displayed)
# Tracking the path for plotting
xnext <- x0
algorithm_path <-
data.frame(step = 1:max_steps,
x = numeric(max_steps),
f = numeric(max_steps))
step <- 1
algorithm_path$x[step] <- xnext
algorithm_path$f[step] <- f(xnext)
# Main loop of gradient descent
while (step < max_steps) {
xlast <- xnext
step <- step + 1
# Updating x, notice the step size
xnext <- xlast - gamma^step * grad_f(xlast)
# Tracking x and f(x) for plot
algorithm_path$x[step] <- xnext
algorithm_path$f[step] <- f(xnext)
}
ggplot() +
xlim(-x0 -.5, x0 +.5) +
# Show f(x)
geom_function(fun = f) +
# Show alg. path
geom_path(aes(x, f), linetype = "dashed",
data = algorithm_path) +
geom_point(aes(x, f, color = step),
data = algorithm_path, size = 2) +
scale_color_binned(type = "viridis", n.breaks = 8, end = .9)
gamma <- .55
algorithm_path <-
data.frame(step = 1:max_steps,
x = numeric(max_steps),
f = numeric(max_steps))
step <- 1
algorithm_path$x[step] <- x0
algorithm_path$f[step] <- f(x0)
while (step < max_steps) {
xlast <- algorithm_path$x[step]
step <- step + 1
xnext <- xlast - gamma^step * grad_f(xlast)
algorithm_path$x[step] <- xnext
algorithm_path$f[step] <- f(xnext)
}
ggplot() +
xlim(-x0 -.5, x0 +.5) +
geom_function(fun = f) +
geom_path(aes(x, f), linetype = "dashed",
data = algorithm_path) +
geom_point(aes(x, f, color = step),
data = algorithm_path, size = 2) +
scale_color_binned(type = "viridis", n.breaks = 8, end = .9)
grad_f(xnext) # numerically close to zero?
gamma^step * grad_f(xnext)
gamma <- c(rep(.95, 5), rep(.9, 5), rep(.85, 5), rep(.8, 10))
data.frame(step = 1:25, gamma = gamma) |>
mutate(step_size = gamma^step) |>
ggplot(aes(x = step, y = step_size)) + geom_point(size = 2)
# Tracking the path for plotting
algorithm_path <-
data.frame(step = 1:max_steps,
x = numeric(max_steps),
f = numeric(max_steps))
step <- 1
algorithm_path$x[step] <- x0
algorithm_path$f[step] <- f(x0)
# Main loop of gradient descent
while (step < max_steps) {
xlast <- algorithm_path$x[step]
step <- step + 1
# Updating x, notice the step size
xnext <- xlast - gamma[step]^step * grad_f(xlast)
# Tracking x and f(x) for plot
algorithm_path$x[step] <- xnext
algorithm_path$f[step] <- f(xnext)
}
# Plot results
ggplot() +
xlim(-x0-.5, x0+.5) +
# Show f(x)
geom_function(fun = f) +
# Show alg. path
geom_path(aes(x, f), linetype = "dashed",
data = algorithm_path) +
geom_point(aes(x, f, color = step),
data = algorithm_path, size = 2) +
scale_color_binned(type = "viridis", n.breaks = 8, end = .9)
# Setting parameters
set.seed(1)
# Try changing some parameters
n <- 200
p <- 10
sparsity <- p/2
nonzero_beta <- runif(sparsity, min = -1, max = 1)
# or, e.g. rep(1, sparsity), runif(sparsity, min = -1, max = 1)
true_beta <- c(.5, nonzero_beta, rep(0, p - sparsity))
# Generating simulated data
X <- cbind(rep(1, n), matrix(rnorm(n*p), nrow = n))
mu <- X %*% true_beta
px <- exp(mu)/(1+exp(mu))
Y <- rbinom(n, 1, prob = px)
train_ld <- data.frame(y = as.factor(Y), x = X)
fit_glm <- glm(Y ~ X-1, family = "binomial")
# to compare with our results later
max_steps <- 50 # try ~3 for comparison
beta_prev2 <- rnorm(ncol(X)) # random start
#beta_prev2 <- c(mean(Y), cor(X[,-1], Y)) # "warm" start
grad_prev2 <- numeric_grad(X, Y, beta_prev2)
# Note: This value of h is chosen to be
# on the order of magnitude of the cube-root
# of .Machine$double.eps, which helps avoid
# some numerical approximation errors
numeric_grad <- function(X, Y, beta, h = 1e-06) {
# Approximate each coordinate of the gradient
numerator <- sapply(1:length(beta), function(j) {
H <- rep(0, length(beta))
H[j] <- h # step in each coordinate j
neglogL(X, Y, beta + H) - neglogL(X, Y, beta - H)
})
numerator / (2*h)
}
# Setting parameters
set.seed(1)
# Try changing some parameters
n <- 200
p <- 10
sparsity <- p/2
nonzero_beta <- runif(sparsity, min = -1, max = 1)
# or, e.g. rep(1, sparsity), runif(sparsity, min = -1, max = 1)
true_beta <- c(.5, nonzero_beta, rep(0, p - sparsity))
# Generating simulated data
X <- cbind(rep(1, n), matrix(rnorm(n*p), nrow = n))
mu <- X %*% true_beta
px <- exp(mu)/(1+exp(mu))
Y <- rbinom(n, 1, prob = px)
train_ld <- data.frame(y = as.factor(Y), x = X)
fit_glm <- glm(Y ~ X-1, family = "binomial")
# to compare with our results later
max_steps <- 50 # try ~3 for comparison
beta_prev2 <- rnorm(ncol(X)) # random start
#beta_prev2 <- c(mean(Y), cor(X[,-1], Y)) # "warm" start
grad_prev2 <- numeric_grad(X, Y, beta_prev2)
# See e.g. lecture slides
neglogL <- function(X, Y, beta) {
Xbeta <- X %*% beta
expXbeta <- exp(-Xbeta)
exp_ratio <- 1/(1+expXbeta)
-sum(Y * log(exp_ratio) + (1-Y) * log(1-exp_ratio))
}
max_steps <- 50 # try ~3 for comparison
beta_prev2 <- rnorm(ncol(X)) # random start
#beta_prev2 <- c(mean(Y), cor(X[,-1], Y)) # "warm" start
grad_prev2 <- numeric_grad(X, Y, beta_prev2)
beta_prev1 <- beta_prev2 + 0.1 * grad_prev2 / sqrt(sum(grad_prev2^2)) #some ascending update
grad_prev1 <- numeric_grad(X, Y, beta_prev1)
previous_loss <- neglogL(X, Y, beta_prev2)
next_loss <- neglogL(X, Y, beta_prev1)
steps <- 1
tol <- 1e-5
while (abs(previous_loss - next_loss) > tol) {
# Compute BB step size
grad_diff <- grad_prev1 - grad_prev2
step_BB <- abs(sum((beta_prev1 - beta_prev2) * grad_diff)) / sum(grad_diff^2)
beta_prev2 <- beta_prev1 # for the (n+1)-th step, beta_{n} becomes beta_{n-1}
# Update step
beta_prev1 <- beta_prev1 - step_BB * grad_prev1 # update the newest parameter
# Shift previous steps
grad_prev2 <- grad_prev1
grad_prev1 <- numeric_grad(X, Y, beta_prev1)
previous_loss <- next_loss
next_loss <- neglogL(X, Y, beta_prev1)
# Print beta values and loss for the first 5 steps
if (steps <= 5) {
cat("Step ", steps, ":\n")
cat("Beta_prev2: ", beta_prev2, "\n")
cat("Beta_prev1: ", beta_prev1, "\n")
cat("Loss: ", next_loss, "\n\n")
}
# Print loss every 5 steps, track path, control loop
if (round(steps / 5) == steps / 5) print(previous_loss)
steps <- steps + 1
beta_path[steps, 2] <- next_loss
beta_path[steps, 3:ncol(beta_path)] <- beta_prev1
if (steps > max_steps) break
}
# Store coefficient path
beta_mat <- rbind(beta_prev2,
beta_prev1,
matrix(0, nrow = max_steps - 1, ncol = ncol(X)))
rownames(beta_mat) <- NULL
beta_path <- data.frame(step = 0:max_steps,
neglogL = c(previous_loss, next_loss, rep(0, max_steps - 1)),
b = beta_mat) |>
rename_with(~str_remove(.x, 'b.'))
tol <- 1e-5
while (abs(previous_loss - next_loss) > tol) {
# Compute BB step size
grad_diff <- grad_prev1 - grad_prev2
step_BB <- abs(sum((beta_prev1 - beta_prev2) * grad_diff)) / sum(grad_diff^2)
beta_prev2 <- beta_prev1 # for the (n+1)-th step, beta_{n} becomes beta_{n-1}
# Update step
beta_prev1 <- beta_prev1 - step_BB * grad_prev1 # update the newest parameter
# Shift previous steps
grad_prev2 <- grad_prev1
grad_prev1 <- numeric_grad(X, Y, beta_prev1)
previous_loss <- next_loss
next_loss <- neglogL(X, Y, beta_prev1)
# Print beta values and loss for the first 5 steps
if (steps <= 5) {
cat("Step ", steps, ":\n")
cat("Beta_prev2: ", beta_prev2, "\n")
cat("Beta_prev1: ", beta_prev1, "\n")
cat("Loss: ", next_loss, "\n\n")
}
# Print loss every 5 steps, track path, control loop
if (round(steps / 5) == steps / 5) print(previous_loss)
steps <- steps + 1
beta_path[steps, 2] <- next_loss
beta_path[steps, 3:ncol(beta_path)] <- beta_prev1
if (steps > max_steps) break
}
beta_path[-1, ] |>
ggplot(aes(step, neglogL)) +
geom_point(size = 2) +
scale_x_continuous(breaks = (1:4) * floor(steps/4)) +
ggtitle("Function value at each step")
distances <- sapply(2:steps, function(step) {
sqrt(sum((beta_path[step, -c(1:2)] - beta_path[step-1, -c(1:2)])^2))
})
data.frame(step = 1:length(distances),
distance = distances) |>
ggplot(aes(step, distance)) +
scale_x_continuous(breaks = (1:4) * floor(steps/4)) +
geom_point(size = 2) + ggtitle("Euclidean distance traversed per update")
set.seed(1) # comment/uncomment/change this
n <- 100
p <- 400
X <- matrix(rnorm(n*p), nrow = n)
beta <- rpois(p, lambda = 1.5)
SGD_starting_beta <- rnorm(p) # generate once
#beta <- sample(c(-1, 0, 0, 1), p, replace = TRUE)
sigma <- 2
y <- X %*% beta + rnorm(n, sd = sigma)
# sparsity level
sum(beta != 0)
# Loss function
least_squares_loss <- function(x, y, beta) {
residuals <- y - x %*% beta
sum(residuals^2)
}
# Gradient of the loss function
least_squares_gradient <- function(x, y, beta) {
residuals <- y - x %*% beta
-2 * t(x) %*% residuals
}
batch_size <- 10
batch_indices <- sample(1:nrow(X), batch_size)
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
gradient_batch <- least_squares_gradient(X_batch, y_batch, SGD_starting_beta)
dim(X_batch)
length(y_batch)
dim(gradient_batch)
epochs <- 40 # decrease for early stopping?
batch_size <- 8 # for batch SGD
gamma <- 0.9
beta_hat <- SGD_starting_beta # generated once above
#beta_hat <- rnorm(p) # random start
num_batches <- ceiling(n/batch_size)
steps <- epochs * num_batches
SGD_path <- data.frame(step = 1:steps,
epoch = numeric(steps),
gamma = numeric(steps),
loss = numeric(steps),
MSE = numeric(steps),
beta_hat_norm = numeric(steps),
gradient_norm = numeric(steps))
start_time <- Sys.time()
step <- 1
for (epoch in 1:epochs) {
# Pass over sample in a random order each time
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
# Update beta_hat
gradient_batch <- least_squares_gradient(X_batch, y_batch, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - gamma^epoch * gradient_batch / (1e-8 + gradient_norm)
LS_loss <- least_squares_loss(X, y, beta_hat)
# Store algorithm path information
SGD_path[step, ] <- c(step,
epoch,
gamma^epoch,
LS_loss,
mean((beta_hat - beta)^2),
sqrt(sum(beta_hat^2)),
gradient_norm)
step <- step + 1
}
}
print(Sys.time() - start_time)
SGD_epoch <- SGD_path |>
select(-step) |>
group_by(epoch) |>
summarize(across(where(is.numeric), mean))
SGD_epoch |>
ggplot(aes(epoch, gamma)) + geom_point() +
ylab("gamma^epoch")
SGD_epoch |>
ggplot(aes(epoch, loss)) + geom_point() +
ylab("Loss") + scale_y_log10()
SGD_epoch |>
ggplot(aes(epoch, MSE)) + geom_point() +
ylab("MSE(beta_hat)")
SGD_epoch |>
ggplot(aes(epoch, beta_hat_norm)) + geom_point() +
ylab("norm(beta_hat)")
SGD_epoch |>
ggplot(aes(epoch, gradient_norm)) + geom_point() +
ylab("norm(gradient)")
set.seed(123)
n <- 500
# Generate Feature1 and Feature2 for Class 0
feature1_class0 <- rnorm(n, mean = 2, sd = 1)
feature2_class0 <- rnorm(n, mean = 2, sd = 1)
# Generate Feature1 and Feature2 for Class 1
feature1_class1 <- rnorm(n, mean = 4, sd = 1)
feature2_class1 <- rnorm(n, mean = 4, sd = 1)
# Combine into data frames
data_class0 <- data.frame(
Feature1 = feature1_class0,
Feature2 = feature2_class0,
Class = 0
)
data_class1 <- data.frame(
Feature1 = feature1_class1,
Feature2 = feature2_class1,
Class = 1
)
# Combine both classes into one dataset
data <- rbind(data_class0, data_class1)
# Introduce some overlap by adding noise
data$Feature1 <- data$Feature1 + rnorm(nrow(data), mean = 0, sd = 0.5)
data$Feature2 <- data$Feature2 + rnorm(nrow(data), mean = 0, sd = 0.5)
# Shuffle the dataset
data <- data[sample(nrow(data)), ]
ggplot(data, aes(x = Feature1, y = Feature2, color = factor(Class))) +
geom_point(alpha = 0.7) +
labs(title = "Simulated Binary Classification Dataset",
x = "Feature 1",
y = "Feature 2",
color = "Class") +
theme_minimal()
train_proportion <- 0.8
train_indices <- sample(1:nrow(data), size = train_proportion * nrow(data))
# Create training and testing sets
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:epochs) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, X_batch)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
final_weights <- result$weights
loss_history <- result$loss_history
# Display final weights
cat("Final Weights:\n")
print(final_weights)
