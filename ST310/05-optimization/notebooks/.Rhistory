beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:1) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
#print(-colMeans( (y_batch-y_hat)*X_batch ))
print(y_batch*X_batch)
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:1) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
#print(-colMeans( (y_batch-y_hat)*X_batch ))
print(y_hat*X_batch)
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:1) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
#print(-colMeans( (y_batch-y_hat)*X_batch ))
print(X_batch)
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:epochs) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
print(-colMeans( (y_batch-y_hat)*X_batch ))
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- sigmoid(z_batch)
print(-colMeans( (y_batch-y_hat)*X_batch ))
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- sigmoid(z_batch)
print(y_hat*X_batch)
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
print(y_hat*X_batch)
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, beta_hat)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:1) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
print(y_hat*X_batch)
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, X_batch)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
final_weights <- result$weights
loss_history <- result$loss_history
# Display final weights
cat("Final Weights:\n")
print(final_weights)
batch_size = 32
sigmoid <- function(z) {
1 / (1 + exp(-z))
}
BCE_loss <- function(y_true, y_pred) {
-mean(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
}
logistic_gradient <- function(y_true, y_pred, X){
-colMeans( (y_true-y_pred)*X )
}
sgd_logistic_regression <- function(train_data, lr = 0.01, epochs = 100) {
X <- as.matrix(train_data[, c("Feature1", "Feature2")])
y <- train_data$Class
# Add intercept term
X <- cbind(Intercept = 1, X)
# Initialize weights
beta_hat <- rep(0, ncol(X))
n <- nrow(X)
loss_history <- numeric(epochs)
num_batches <- ceiling(n/batch_size)
for (epoch in 1:epochs) {
shuffled_indices <- sample(1:n, n, replace = TRUE)
for (batch in 1:num_batches) {
batch_start <- 1 + batch_size * (batch - 1)
batch_end <- min(batch_size * batch, nrow(X))
batch_indices <- shuffled_indices[batch_start:batch_end]
# Mini-batch
X_batch <- X[batch_indices, , drop = FALSE]
y_batch <- y[batch_indices]
z_batch <- X_batch %*% beta_hat
y_hat <- as.vector(sigmoid(z_batch))
# Update beta_hat
gradient_batch <- logistic_gradient(y_batch, y_hat, X_batch)
gradient_norm <- sqrt(sum(gradient_batch^2))
beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
entropy_loss <- BCE_loss(y_batch, y_hat)
}
if (epoch %% 10 == 0 || epoch == 1) {
cat("Epoch:", epoch, "- Loss:", round(entropy_loss, 4), "\n")
}
}
return(list(weights = beta_hat, loss_history = loss_history))
}
# Train the model
result <- sgd_logistic_regression(train_data, lr = 0.1, epochs = 100)
final_weights <- result$weights
loss_history <- result$loss_history
# Display final weights
cat("Final Weights:\n")
print(final_weights)
X_test <- as.matrix(test_data[, c("Feature1", "Feature2")])
y_test <- test_data$Class
# Add intercept term
X_test <- cbind(Intercept = 1, X_test)
# Compute predictions
z_test <- X_test %*% final_weights
y_hat_test <- sigmoid(z_test)
predictions_test <- ifelse(y_hat_test >= 0.5, 1, 0)
# Calculate accuracy
accuracy_test <- mean(predictions_test == y_test)
beta0 <- final_weights[1]
beta1 <- final_weights[2]
beta2 <- final_weights[3]
# Define decision boundary function
decision_boundary <- function(x1) {
-(beta0 + beta1 * x1) / beta2
}
x1_vals <- seq(min(train_data$Feature1) - 1, max(train_data$Feature1) + 1, length.out = 100)
x2_vals <- decision_boundary(x1_vals)
ggplot(test_data, aes(x = Feature1, y = Feature2, color = factor(Class))) +
geom_point(alpha = 0.7) +
# Add the decision boundary using a separate data frame
geom_line(data = data.frame(Feature1 = x1_vals, Feature2 = x2_vals),
aes(x = Feature1, y = Feature2),
color = "black", size = 1) +
labs(title = "Decision Boundary on Testing Data",
x = "Feature 1",
y = "Feature 2",
color = "Class") +
theme_minimal()
