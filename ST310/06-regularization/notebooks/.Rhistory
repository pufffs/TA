knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
theme_set(theme_minimal(base_size = 14))
library(caret)     # For cross-validation and model training
#library(caret)     # For cross-validation and model training
library(ggplot2)   # For visualization
library(dplyr)     # For data manipulation
set.seed(123)
n <- 1000
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)  # Randomly generated features
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)         # Simulated response with some noise
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)  # 80% training data
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.01, 1, length = 10)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5  # Number of folds
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$rmse <- NA
grid_results
rep(1:k, length.out = nrow(X_train))
folds
#library(caret)
library(ggplot2)
library(dplyr)
set.seed(123)
n <- 1000
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.1, 1, length = 3)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$rmse <- NA
#library(caret)
library(ggplot2)
library(dplyr)
set.seed(123)
n <- 500
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.1, 1, length = 3)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$rmse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
rmse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
# Calculate RMSE for the fold
rmse_fold <- sqrt(mean((predictions - y_valid_fold)^2))
rmse_folds <- c(rmse_folds, rmse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$rmse[i] <- mean(rmse_folds)
}
install.packages("caret")
library(caret)
library(ggplot2)
library(dplyr)
set.seed(123)
n <- 500
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.1, 1, length = 3)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$rmse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
rmse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
# Calculate RMSE for the fold
rmse_fold <- sqrt(mean((predictions - y_valid_fold)^2))
rmse_folds <- c(rmse_folds, rmse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$rmse[i] <- mean(rmse_folds)
}
library(glmnet)
install.packages("glmnet")
library(caret)
library(ggplot2)
library(dplyr)
library(glmnet)
set.seed(123)
n <- 500
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.1, 1, length = 3)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$rmse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
rmse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
# Calculate RMSE for the fold
rmse_fold <- sqrt(mean((predictions - y_valid_fold)^2))
rmse_folds <- c(rmse_folds, rmse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$rmse[i] <- mean(rmse_folds)
}
# Step 5: Find Best Parameters
best_params <- grid_results %>% filter(rmse == min(rmse))
cat("Best Parameters:\n")
print(best_params)
# Step 6: Train Final Model with Best Parameters and Evaluate on Test Set
best_alpha <- best_params$alpha[1]
best_lambda <- best_params$lambda[1]
final_model <- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda)
# Step 7: Evaluate Performance on Test Set
predictions <- predict(final_model, X_test)
test_rmse <- sqrt(mean((predictions - y_test)^2))
cat("Test Set RMSE:", test_rmse, "\n")
grid_results
grid_results[1]
grid_results[1,:]
grid_results
grid_results$test_rmse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
rmse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
# Calculate RMSE for the fold
rmse_fold <- sqrt(mean((predictions - y_valid_fold)^2))
rmse_folds <- c(rmse_folds, rmse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$rmse[i] <- mean(rmse_folds)
# Train the model on the entire training set and evaluate on the test set
final_model <- glmnet(X_train, y_train, alpha = alpha, lambda = lambda)
test_predictions <- predict(final_model, X_test)
grid_results$test_rmse[i] <- sqrt(mean((test_predictions - y_test)^2))
}
# Step 5: Find Best Parameters
best_params <- grid_results %>% filter(rmse == min(rmse))
cat("Best Parameters:\n")
print(best_params)
# Step 6: Train Final Model with Best Parameters and Evaluate on Test Set
best_alpha <- best_params$alpha[1]
best_lambda <- best_params$lambda[1]
final_model <- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda)
# Step 7: Evaluate Performance on Test Set
predictions <- predict(final_model, X_test)
test_rmse <- sqrt(mean((predictions - y_test)^2))
cat("Test Set RMSE:", test_rmse, "\n")
grid_results
print(grid_results)
grid_results$test_rmse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
rmse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
# Calculate RMSE for the fold
rmse_fold <- sqrt(mean((predictions - y_valid_fold)^2))
rmse_folds <- c(rmse_folds, rmse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$rmse[i] <- mean(rmse_folds)
# Train the model on the entire training set and evaluate on the test set
final_model <- glmnet(X_train, y_train, alpha = alpha, lambda = lambda)
test_predictions <- predict(final_model, X_test)
grid_results$test_rmse[i] <- sqrt(mean((test_predictions - y_test)^2))
}
print(grid_results)
library(caret)
library(ggplot2)
library(dplyr)
library(glmnet)
set.seed(123)
n <- 500
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.1, 1, length = 3)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$mse <- NA
grid_results$test_mse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
mse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
mse_fold <- mean((predictions - y_valid_fold)^2)
mse_folds <- c(mse_folds, mse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$mse[i] <- mean(mse_folds)
# Train the model on the entire training set and evaluate on the test set
final_model <- glmnet(X_train, y_train, alpha = alpha, lambda = lambda)
test_predictions <- predict(final_model, X_test)
grid_results$test_rmse[i] <- sqrt(mean((test_predictions - y_test)^2))
}
print(grid_results)
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
mse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
mse_fold <- mean((predictions - y_valid_fold)^2)
mse_folds <- c(mse_folds, mse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$mse[i] <- mean(mse_folds)
# Train the model on the entire training set and evaluate on the test set
final_model <- glmnet(X_train, y_train, alpha = alpha, lambda = lambda)
test_predictions <- predict(final_model, X_test)
grid_results$test_mse[i] <- sqrt(mean((test_predictions - y_test)^2))
}
print(grid_results)
library(caret)
library(ggplot2)
library(dplyr)
library(glmnet)
set.seed(123)
n <- 500
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
y <- 3 * X[, 1] - 2 * X[, 2] + rnorm(n)
dataset <- data.frame(X, y)
# Step 2: Split Data into Training and Test Sets
trainIndex <- sample(1:n, size = 0.8 * n)
trainData <- dataset[trainIndex, ]
testData <- dataset[-trainIndex, ]
X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$y
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$y
# Step 3: Define Grid Search Parameters
alpha_values <- c(0, 0.5, 1)  # Tuning alpha (0: Ridge, 1: Lasso, 0.5: Elastic Net)
lambda_values <- seq(0.1, 1, length = 3)  # Tuning lambda (regularization parameter)
# Step 4: Manual Cross-Validation with Grid Search
k <- 5
folds <- sample(rep(1:k, length.out = nrow(X_train)))
grid_results <- expand.grid(alpha = alpha_values, lambda = lambda_values)
grid_results$mse <- NA
grid_results$test_mse <- NA
for (i in 1:nrow(grid_results)) {
alpha <- grid_results$alpha[i]
lambda <- grid_results$lambda[i]
mse_folds <- c()
for (fold in 1:k) {
X_train_fold <- X_train[folds != fold, ]
y_train_fold <- y_train[folds != fold]
X_valid_fold <- X_train[folds == fold, ]
y_valid_fold <- y_train[folds == fold]
# Fit model with current alpha and lambda
model <- glmnet(X_train_fold, y_train_fold, alpha = alpha, lambda = lambda)
predictions <- predict(model, X_valid_fold)
mse_fold <- mean((predictions - y_valid_fold)^2)
mse_folds <- c(mse_folds, mse_fold)
}
# Store average RMSE for the current parameter combination
grid_results$mse[i] <- mean(mse_folds)
# Train the model on the entire training set and evaluate on the test set
final_model <- glmnet(X_train, y_train, alpha = alpha, lambda = lambda)
test_predictions <- predict(final_model, X_test)
grid_results$test_mse[i] <- sqrt(mean((test_predictions - y_test)^2))
}
print(grid_results)
model <- trainControl(method = "cv", number = 5)
grid <- expand.grid(
alpha = c(0, 0.5, 1),
lambda = seq(0.1, 1, length = 3)
)
model_fit <- train(
y ~ .,
data = trainData,
method = "glmnet",
trControl = model,
tuneGrid = grid
)
best_params <- model_fit$bestTune
cat("Best Parameters:\n")
print(best_params)
predictions <- predict(model_fit, newdata = testData)
mse <- mean((predictions - testData$y)^2)
cat("Test Set RMSE:", mse, "\n")
plot(model_fit)
