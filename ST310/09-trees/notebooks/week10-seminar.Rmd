
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)  
library(modelr)     
library(gapminder)  
theme_set(theme_minimal(base_size = 22))
```

## Sorting preliminaries

Check the documentation for `?sort` and `?order` and then write code to output the values of `Y` sorted according to the order of `X`

```{r}
X <- runif(10)
Y <- X + rnorm(10)
qplot(X, Y)
```

Base R solution

```{r}
Y[order(X)]
```


#### Check with tidyverse solution

```{r}
egdf <- data.frame(X = X, Y = Y)
egdf %>%
  arrange(X)
```

## Within-leaf averages

Below is some code that computes the average values of `Y` above and below a given split point

Base R

```{r}
x_split <- 0.5
c(mean(Y[X <= x_split]),
  mean(Y[X > x_split]))
```

tidyverse

```{r}
egdf %>%
  group_by(X <= x_split) %>%
  summarize(avg_Y = mean(Y))
```

#### How much computation is required if we change the value of `x_split`?

Have to re-sort the data to find the indexes of `Y` corresponding to `X` values above/below the new split point

(Sorting a list of `k` items requires--in the worst case--an order of `k*log(k)` operations)

#### Write code that sorts the data on `X` only once, and then, taking each `X` value as a split point consecutively, computes the average `Y` values above and below that split point while minimizing unnecessary computation

```{r}
x_order <- order(X)
Y_sorted <- Y[x_order]
n <- length(X)
for (i in 1:(n-1)) {
  print(c(mean(Y_sorted[1:i]),
  mean(Y_sorted[(i+1):n])))
}
```

#### How can we use this to decide the split point for a regression tree?

We can use the average `Y` values as predictions within each leaf, compute the RSS, and choose the split point giving the lowest RSS

#### How could we change this code to guarantee a minimum number of observations in each leaf?

Instead of making the loop go from 1 to `(n-1)`, we can make it start at `min.obs` and end at `(n - min.obs)`. **Here we only consider the left side of the node, how about the right side?** If we have 11 data points, start from 10, then left side has 10 points, right side has only 1.

## Numeric predictor

Write a function that inputs a single numeric predictor and outcome, and outputs a splitting point that achieves the lowest RSS

```{r}
tree_split_wrong <- function(x, y, min.obs = 10) {
  x_order <- order(x)
  X <- x[x_order]
  Y <- y[x_order]
  n <- length(x)
  RSSs <- numeric(length = n-1)
  RSSs[1:length(RSSs)] <- Inf
  for (i in min.obs:(n-min.obs)) {
    Y_left <- Y[1:i]
    Y_right <- Y[(i+1):n]
    RSSs[i] <- sum((Y_left - mean(Y_left))^2) + 
      sum((Y_right - mean(Y_right))^2)
  }
  X[which.min(RSSs)]
}

tree_split <- function(x, y, min.obs = 10) {
  x_order <- order(x)
  X <- x[x_order]
  Y <- y[x_order]
  n <- length(x)
  if (n>min.obs){
    RSSs <- numeric(length = n-1)
    RSSs[1:length(RSSs)] <- Inf
  for (i in 1:(n-1)) {
    Y_left <- Y[1:i]
    Y_right <- Y[(i+1):n]
    RSSs[i] <- sum((Y_left - mean(Y_left))^2) + 
      sum((Y_right - mean(Y_right))^2)
  }
  X[which.min(RSSs)]
  } else {return ()}
}
```

### Example data

```{r}
n <- 1000
mixture_ids <- rbinom(n, 1, .5)
x <- rnorm(n) + 3*mixture_ids
y <- rnorm(n) + 3*mixture_ids
qplot(x,y)
```

#### Apply your function to the example data. Try different values of minimum observations per leaf

```{r}
tree_split(x, y, min.obs = 10)
```

#### Change the data generating process to make the example easier/harder and repeat the above initial split

```{r}
n <- 1000
mixture_ids <- rbinom(n, 1, .5)
x <- rnorm(n) + 2*mixture_ids
y <- rnorm(n) + 1*mixture_ids
qplot(x,y)
```

```{r}
tree_split(x, y, min.obs = 10)
```

#### Compare the initial tree split to a Bayes decision boundary in one of the above examples. Increase `n` and repeat

#### Test the function on some `gapminder` data, plot the initial split point

```{r}
gm2007 <- gapminder %>% filter(year == 2007)
split2007 <- with(gm2007, tree_split(gdpPercap, lifeExp))
split2007
```

```{r}
gm2007 %>%
  ggplot(aes(gdpPercap, lifeExp)) + 
  geom_point() +
  geom_vline(xintercept = split2007)
```


# Extra practice

```{r}
n <- 1000
mixture_ids <- rbinom(n, 1, .5)
x <- rnorm(n) + 3*mixture_ids
y <- rnorm(n) + 3*mixture_ids
x <- c(x, rnorm(n/2, mean = -2))
y <- c(y, rnorm(n/2, mean = 5))
egdf <- data.frame(x = x, y = y)
egplot <- egdf %>%
  ggplot(aes(x, y)) +
  geom_point()
egplot
```

## Multiple splits

#### Use your single split function once, then split the data and apply the function again on each subset

```{r}
split1 <- tree_split(x, y)
split1
```

```{r}
inds1 <- which(x <= split1)
inds2 <- setdiff(1:length(x), inds1)
```

```{r}
split2 <- tree_split(x[inds1], y[inds1])
split2
```

```{r}
split3 <- tree_split(x[inds2], y[inds2])
split3
```

#### Plot the data and each split point

```{r}
egplot +
  geom_vline(xintercept = split1) +
  geom_vline(xintercept = split2) +
  geom_vline(xintercept = split3) 
```


## Recursive splits

### Recursive algorithm

```{r}
# Recursive algorithm to calculate factorial
factorial_recursive <- function(n) {
  if (n == 0) {
    return(1)
  } else {
    return(n * factorial_recursive(n - 1))
  }
}

result <- factorial_recursive(5)
print(result) 

```
#### Write a function taking data and a maximum number of splits as inputs, and outputting a decision tree
```{r}

# Hint: In this function, consider recursively apply the function to the left and right sides of the node.
recursive_split <- function(x,y, min_obs, max_split=5){
  if (max_split == 0 || length(x) <= min_obs) {
      return() }
  split <- tree_split(x,y,min_obs)
  splits <- c(split)
  left_ind <- which(x <= split)
  left_x <- x[left_ind]
  left_y <- y[left_ind]
  
  right_ind <- which(x > split)
  right_x <- x[right_ind]
  right_y <- y[right_ind]
  
  splits_left <- recursive_split(left_x, left_y, min_obs, max_split-1)
  splits_right <- recursive_split(right_x, right_y, min_obs, max_split-1)
  splits <- c(splits_left, splits, splits_right)
  splits
}


tree_predict <- function(splits, x_test, x, y){
  m <- length(splits)
  predictions <- c()
  for (x_0 in x_test){
    if (x_0<=splits[1]){
      y_0 <- mean(y[x<=splits[1]])
    } else if (x_0>splits[m]){
      y_0 <- mean(y[x>splits[m]])
    } else {
      for (i in 1:(m-1)){
        left <- splits[i]
        right <- splits[i+1]
        if ( x_0<=right & x_0>left ){
          y_0 <- mean(y[x<=right&x>left])
          break
        }
      }
    }
  predictions <- c(predictions, y_0)
  }
  predictions
}
```


```{r}
splits <- recursive_split(x,y,10,2)
tree_predict(splits, c(1,2,3),x,y)
```



## Classification Trees

#### Write a function for when the outcome variable is categorical and some splitting rule based on e.g. Gini index or entropy
$$G=\sum_{k=1}^K\hat p_{mk}(1-p_{mk})=\sum_{k=1}^K\hat p_{mk}-p_{mk}^2=1-\sum_{k=1}^Kp_{mk}^2$$
```{r}
classifier_split <- function(x, y, min.obs = 10) {
  x_order <- order(x)
  X <- x[x_order]
  Y <- y[x_order]
  n <- length(x)
  if (n > min.obs) {
    Ginis <- numeric(length = n - 1)
    Ginis[1:length(Ginis)] <- Inf
    for (i in 1:(n - 1)) {
      Y_left <- Y[1:i]
      Y_right <- Y[(i + 1):n]
      
      p_left <- table(Y_left) / length(Y_left)
      p_right <- table(Y_right) / length(Y_right)
      
      Gini_left <- 1 - sum(p_left^2)
      Gini_right <- 1 - sum(p_right^2)
      
      Ginis[i] <- (length(Y_left) / n) * Gini_left + (length(Y_right) / n) * Gini_right
    }
    X[which.min(Ginis)]
  } else {
    return()
  }
}
```

#### An example
```{r}
n <- 100
x <- runif(n)
noise <- rbinom(n, 1, 0.1)  
y <- ifelse(x > 0.5, 1, 0)
y <- ifelse(noise == 1, 1 - y, y) 
qplot(x,y)
```
```{r}
classifier_split(x,y)
```

