{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "governmental-penalty",
   "metadata": {},
   "source": [
    "## LSE ST455 Reinforcement Learning  - Alpha Zero for Gomoku\n",
    "\n",
    "In this week's Seminar we look at an implementation of [Alpha Zero](https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd). Technicall, we loook at its predecessor, a version specialised to Go called [AlphaGo Zero](https://www.nature.com/articles/nature24270).\n",
    "\n",
    "In the lecture in Week 9 we spoke about AlphaGo. This old version of a reinforcement learning method for Go did still use expert games as supervised learning. Alpha Zero instead learns solely through self-play. This architecture can therefore be applied to vastly different games and environments as long as the rules are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-census",
   "metadata": {},
   "source": [
    "## AlphaGo Zero\n",
    "\n",
    "AlphaGo Zero uses a joint network for predicting the network value function $V(s)$ and the action probabilities $p(s)$ given a specific board state $s$. In a Monte-Carlo-Tree-Search(MCTS) where the root is the current state we use current action probabilities are used to generate an exploration tree.\n",
    "\n",
    "### Monte-Carlo-Tree-Search\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"images/MCTS.png\" width=800>\n",
    "    <figcaption>\n",
    "      Taken from DeepMind. Great depiction of MCTS.\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n",
    "\n",
    "Once the Tree Search is finished we collect tree search values $z$ and tree search probabilities $\\pi$ for all the nodes in the search tree.\n",
    "\n",
    "### Self-Play\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"images/self_play.png\" width=800>\n",
    "    <figcaption>\n",
    "      Taken from DeepMind\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n",
    "\n",
    "The model plays against itself which is depicted above. Given a certain game state the algorithm performs a MTCS to generate the tree search probabilities $\\pi$. An action is then sampled from $\\pi$ which brings us in the next state. Again, perform a MTCS and so on..\n",
    "This goes on until the game is finished. The tree search reward $z$ is collected.\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"images/training.png\" width=800>\n",
    "    <figcaption>\n",
    "      Taken from DeepMind\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n",
    "\n",
    "$$L_{value} = (z - V)^2.$$ \n",
    "\n",
    "This is the squared loss induced by wrong prediction of the value functions at the experienced states.\n",
    "\n",
    "The loss corresponding to the actions is \n",
    "\n",
    "$$L_{action} = - \\pi^\\top \\log p.$$\n",
    "\n",
    "The loss is large if the tree search probabilities $\\pi$ are very different from the network probabilities $p$ and is minimized if $\\pi = p$. The function $\\pi$ is generated by the MCTS where the initial update probabilities $p$ are modified to account for experienced Monte-Carlo $Q$ functions and stimulate exploration by using upper confidence bounds (recall our use of these in Seminar 1 when discussing $k$-bandit problems).\n",
    "\n",
    "The regularization loss is defined as \n",
    "\n",
    "$$L_{reg} = c \\|\\theta\\|^2$$\n",
    "\n",
    "for some constant $c$ and the current network parameters $\\theta$.\n",
    "\n",
    "The overall loss is defined as\n",
    "\n",
    "$$L = L_{value} + L_{action} + L_{reg}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-colorado",
   "metadata": {},
   "source": [
    "## Gomoku\n",
    "\n",
    "The implementation we use is applied to the game of [Gomoku](https://en.wikipedia.org/wiki/Gomoku).\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"images/gomoku.jpg\" width=300>\n",
    "    <figcaption>\n",
    "      Typical game state in Gomoku\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n",
    "\n",
    "Gomoku is a 2-player game where players alternatingly place stones on a square board. The aim of the game is to position $k$ consecutive stones horizontally, vertically or diagonally.\n",
    "\n",
    "Gomoku is a much easier game than Go and board sizes up to 10 x 10 are suitable for a standard CPU to develop a good model in a few days.\n",
    "\n",
    "Let us begin by playing a round on an 8x8 board where the target is to get 5 consecutive stones. The MCTS Model used was trained for a few days on a standard CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "patent-month",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from play import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-bundle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "run(width=8, height=8, num_consecutive=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-booth",
   "metadata": {},
   "source": [
    "The current AI good but still far from optimal: \n",
    "    \n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"images/win_against_8_8_5.png\" width=300>\n",
    "    <figcaption>\n",
    "      Implemented Graphical User Interface: Winning against the current 8x8 and 5 consecutive stones.\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-remainder",
   "metadata": {},
   "source": [
    "### The network architecture \n",
    "\n",
    "#### Input\n",
    "\n",
    "```python\n",
    "self.board_width = board_width\n",
    "self.board_height = board_height\n",
    "\n",
    "# Define the tensorflow neural network\n",
    "# 1. Input:\n",
    "self.input_states = tf.placeholder(\n",
    "        tf.float32, shape=[None, 4, board_width, board_height])\n",
    "self.input_state = tf.transpose(self.input_states, [0, 2, 3, 1])\n",
    "```\n",
    "\n",
    "#### Network Layers\n",
    "\n",
    "```python\n",
    "# 2. Common Networks Layers\n",
    "self.conv1 = tf.layers.conv2d(inputs=self.input_state,\n",
    "                              filters=32, kernel_size=[3, 3],\n",
    "                              padding=\"same\", data_format=\"channels_last\",\n",
    "                              activation=tf.nn.relu)\n",
    "self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64,\n",
    "                              kernel_size=[3, 3], padding=\"same\",\n",
    "                              data_format=\"channels_last\",\n",
    "                              activation=tf.nn.relu)\n",
    "self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=128,\n",
    "                              kernel_size=[3, 3], padding=\"same\",\n",
    "                              data_format=\"channels_last\",\n",
    "                              activation=tf.nn.relu)\n",
    "```\n",
    "\n",
    "#### Activation Networks\n",
    "\n",
    "```python\n",
    "# 3-1 Action Networks\n",
    "self.action_conv = tf.layers.conv2d(inputs=self.conv3, filters=4,\n",
    "                                    kernel_size=[1, 1], padding=\"same\",\n",
    "                                    data_format=\"channels_last\",\n",
    "                                    activation=tf.nn.relu)\n",
    "# Flatten the tensor\n",
    "self.action_conv_flat = tf.reshape(\n",
    "        self.action_conv, [-1, 4 * board_height * board_width])\n",
    "# 3-2 Full connected layer, the output is the log probability of moves\n",
    "# on each slot on the board\n",
    "self.action_fc = tf.layers.dense(inputs=self.action_conv_flat,\n",
    "                                 units=board_height * board_width,\n",
    "                                 activation=tf.nn.log_softmax)\n",
    "```\n",
    "\n",
    "#### Evaluation Networks\n",
    " \n",
    " ```python\n",
    "self.evaluation_conv = tf.layers.conv2d(inputs=self.conv3, filters=2,\n",
    "                                        kernel_size=[1, 1],\n",
    "                                        padding=\"same\",\n",
    "                                        data_format=\"channels_last\",\n",
    "                                        activation=tf.nn.relu)\n",
    "self.evaluation_conv_flat = tf.reshape(\n",
    "        self.evaluation_conv, [-1, 2 * board_height * board_width])\n",
    "self.evaluation_fc1 = tf.layers.dense(inputs=self.evaluation_conv_flat,\n",
    "                                      units=64, activation=tf.nn.relu)\n",
    "# output the score of evaluation on current state\n",
    "self.evaluation_fc2 = tf.layers.dense(inputs=self.evaluation_fc1,\n",
    "                                      units=1, activation=tf.nn.tanh)\n",
    "```\n",
    "\n",
    "#### The loss function\n",
    "\n",
    "```python\n",
    "# 1. Label: the array containing if the game wins or not for each state\n",
    "self.labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "# 2. Predictions: the array containing the evaluation score of each state\n",
    "# which is self.evaluation_fc2\n",
    "# 3-1. Value Loss function\n",
    "self.value_loss = tf.losses.mean_squared_error(self.labels,\n",
    "                                               self.evaluation_fc2)\n",
    "# 3-2. Policy Loss function\n",
    "self.mcts_probs = tf.placeholder(\n",
    "        tf.float32, shape=[None, board_height * board_width])\n",
    "self.policy_loss = tf.negative(tf.reduce_mean(\n",
    "        tf.reduce_sum(tf.multiply(self.mcts_probs, self.action_fc), 1)))\n",
    "# 3-3. L2 penalty (regularization)\n",
    "l2_penalty_beta = 1e-4\n",
    "vars = tf.trainable_variables()\n",
    "l2_penalty = l2_penalty_beta * tf.add_n(\n",
    "    [tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name.lower()])\n",
    "# 3-4 Add up to be the Loss function\n",
    "self.loss = self.value_loss + self.policy_loss + l2_penalty\n",
    "\n",
    "```\n",
    "\n",
    "#### The optimizer\n",
    "\n",
    "```python\n",
    "# Define the optimizer we use for training\n",
    "self.learning_rate = tf.placeholder(tf.float32)\n",
    "self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-montana",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "We can also train a new model based on new board dimensions. The best model encountered during training will be stored and can be used to play against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TrainPipeline\n",
    "\n",
    "training_pipeline = TrainPipeline(width=5, height=5, n_in_row=2)\n",
    "training_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-shift",
   "metadata": {},
   "source": [
    "## On a few technical details in the Gomoku Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-parker",
   "metadata": {},
   "source": [
    "### Data augmentation via rotation/flipping\n",
    "\n",
    "The game state is invariant under rotating the board or flipping the game state at one of the axes of symmetry. Hence we can simulate data based on these transformations.\n",
    "\n",
    "```python\n",
    "def get_equi_data(self, play_data):\n",
    "    \"\"\"augment the data set by rotation and flipping\n",
    "    play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
    "    \"\"\"\n",
    "    extend_data = []\n",
    "    for state, mcts_porb, winner in play_data:\n",
    "        for i in [1, 2, 3, 4]:\n",
    "            # rotate counterclockwise\n",
    "            equi_state = np.array([np.rot90(s, i) for s in state])\n",
    "            equi_mcts_prob = np.rot90(np.flipud(\n",
    "                mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
    "            extend_data.append((equi_state,\n",
    "                                np.flipud(equi_mcts_prob).flatten(),\n",
    "                                winner))\n",
    "            # flip horizontally\n",
    "            equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "            equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "            extend_data.append((equi_state,\n",
    "                                np.flipud(equi_mcts_prob).flatten(),\n",
    "                                winner))\n",
    "    return extend_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
